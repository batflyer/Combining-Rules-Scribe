%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing

% Additional Packages following those which are required.
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
}

\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Probabilistic Soft Logic for Semantic Textual Similarity)
/Author (Srijita Das, Navdeep Kaur)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Probabilistic Soft Logic for Semantic Textual Similarity\\
\large Islam Beltagy, Katrin Erk, and Raymond Mooney
}
\author{Srijita Das\\
Indiana University, Bloomington\\
sridas@indiana.edu\\
\And
Navdeep Kaur\\
Indiana University, Bloomington\\
navdkaur@indiana.edu\\
\And
Alexander L. Hayes (scribe)\\
The University of Texas at Dallas\\
alexander.hayes@utdallas.edu
}

\maketitle
\begin{abstract}
\begin{quote}
Srijita Das and Navdeep Kaur present the work of Islam Beltagy, Katrin Erk, and Raymond Mooney: \href{https://www.cs.utexas.edu/~ml/papers/beltagy.acl14.pdf}{``Probabilistic Soft Logic for Semantic Textual Similarity"}. Part I in a presentation series on Learning as part of Professor Sriraam Natarajan's seminar on statistical relational learning (CS 7301.002). Work was presented on Wednesday, February 21, 2018 in ECSS 2.311 at the University of Texas at Dallas.
\end{quote}
\end{abstract}

\section{Summary}

``Textual Similarity'' is usually defined as a set of distance metrics in text classification tasks, with common algorithms including Hamming distance, Levenshtein (edit) distance, or cosine similarity. In a broad sense, these collective algorithms make observations about the occurrence of letters, words, and their ordering; then use these to calculate a similarity score between strings.

``Semantic Textual Similarity'' is the harder problem which attempts to answer how similar two strings are in \textit{meaning}. As a motivating example, a single character can drastically change the meaning of a sentence: ``Let's eat, grandma!'' and ``Let's eat grandma!'' differ by a comma, but the former implies that dinner is about to begin while the latter implies a family's brutal descent into human cannibalism.

The authors propose a learning approach to solving semantic textual similar, where the idea is to model the meaning of a phrase by determining the meaning of its parts and the relationships that connect the parts. They use the framework of ``probabilistic soft logic'' (or \textbf{PSL}), which is an approach that combines logical semantics and distributional semantics into a single representation with tractable inference.

This method was evaluated based on three datasets (msr-vid, msr-par, and SICK) for several variations of the grounding process and compared with Markov Logic Networks. The authors reported CPU time and Pearson correlations across each.

\section{Pros}

\begin{itemize}
    \item The authors define a system which combines distributional and logical semantics of text, giving better insight to the semantic meaning of a sentence than one or the other would alone.
    \item Evidence is defined as a set of scores, which allows sentences to have multiple scores and therefore multiple interpretations. This is powerful since text may have inherent ambiguities.
\end{itemize}

\section{Contributions}

The authors claim the following contributions:

\begin{itemize}
    \item ``We use the same combined logic-based and distribution framework as `Montague Meets Markov: Deep Semantics with Probabilistic Logical Form' (2013), but replace Markov Logic Networks with Probabilistic Soft Logic.''
    \item ``We show how to use PSL for STS, and describe changes to the PSL framework that make it more effective for STS.''
\end{itemize}

These claims are evaluated on three datasets.

\section{Improvements}

Professor Natarajan and Professor Kunapuli point out the following: Probabilistic Soft Logic directly optimizes a similarity score, whereas Markov Logic Networks optimize the log-likelihood. With Pearson correlation as the metric these are evaluated against, PSL should always win since it directly optimizes the correct thing.

``What I would have done, is on one side: the similarity. And the other side is the probability of a sentence being true in the world. Then I could have said that I have two sides of the story, and I can also compare which is true and which is false."---Professor Natarajan

The authors used \href{https://alchemy.cs.washington.edu/}{Alchemy} to evaluate Markov Logic Networks. \href{http://i.stanford.edu/hazy/tuffy/}{Tuffy} would have made more sense as the implementation to use, particularly since it scales better than the former.

\end{document}
