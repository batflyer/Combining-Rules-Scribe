%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing

% Additional Packages following those which are required.
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
}

\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Exploiting Causal Independence in Markov Logic Networks: Combining Directed and Undirected Models)
/Author (Harsha Kokel, Nandini Ramanan)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Exploiting Causal Independence in Markov Logic\\Networks: Combining Directed and Undirected Models\\
\large Sriraam Natarajan, Tushar Khot, Daniel Lowd,\\Kristian Kersting, Prasad Tadepalli, and Jude Shavlik
}
\author{Harsha Kokel\\
The University of Texas at Dallas\\
harsha.kokel@utdallas.edu\\
\And
Nandini Ramanan\\
Indiana University, Bloomington\\
nramanan@indiana.edu\\
}

\maketitle
\begin{abstract}
\begin{quote}
Harsha Kokel and Nandini Ramanan present the work of Sriraam Natarajan, Tushar Khot, Daniel Lowd, Kristian Kersting, Prasad Tadepalli, and Jude Shavlik, \href{http://homes.soic.indiana.edu/natarasr/Papers/Natarajan.ECML10.pdf}{``Exploiting Causal Independence in Markov Logic Networks: Combining Undirected and Directed Models"}. Part IV in a presentation series on Representation as part of Professor Sriraam Natarajan's seminar on statistical relational learning (CS 7301.002). Work was presented on Wednesday, February 7, 2018 in ECSS 2.311 at the University of Texas at Dallas.
\end{quote}
\end{abstract}

\section{Summary}

The aim of this paper is twofold: remedy the representative disparities between directed and undirected models (specifically Bayesian Networks and Markov Logic Networks) with causal independence, and show how decomposable combination functions can be used to augment these in the relational setting. Independence  of causal influences (``ICI'', from Heckerman) can be shown using directed models since they have a coherent joint distribution from which individual sensors can be aggregated. But this comes somewhat at the expense of more rigorous modeling requirements, perhaps most notably the inability to have cycles.

\subsection{Independence of Causal Influences}

Consider two domains: one in which we are considering the values produced by a set of \textit{N} sensors, and another where we are trying to conceive a student's overall satisfaction by observing observations about their grades and reviews of papers they published. Modeling a full conditional probability table (CPT) would normally require $2^n$ values, but if one observes that individual measurements of each sensor are independent of each other one will see the root of the idea of causal independence: there are many values which can influence your target, but when each are likely to be independent of one another then learning the entire CPT is wasteful.

\subsection{Decomposable Combination Functions}

1. What are decomposable combination functions and how do they differ from the wider set of all combination functions?
2. What is the algorithm that converts directed models to undirected models (touch on the cpt rules)?
3. How are the claims of this paper evaluated?

\section{Pros and Cons}

Pros:
Cons: Most notably, this method works best when the number of unobserved variables is small. Even when there is a single unobserved variable, the search space grows exponentially.

\section{Contributions}

The authors claim the following contributions: ``(1) A provable linear representation of decomposable combining functions within MLNs; (2) Explicit examples of average-based and noisy combination functions; (3) A formal description of the algorithm for converting from directed models with combining rules to MLNs; (4) A macro-definition that allows for succinct specification of the resulting MLNs; (5) Empirical proof that combining rules can improve the learning of MLNs when the domain knowledge available is minimal."

\section{Improvements}

\end{document}
