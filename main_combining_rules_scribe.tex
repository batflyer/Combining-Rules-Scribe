%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing

% Additional Packages following those which are required.
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
}

\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Exploiting Causal Independence in Markov Logic Networks: Combining Directed and Undirected Models)
/Author (Harsha Kokel, Nandini Ramanan)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Exploiting Causal Independence in Markov Logic\\Networks: Combining Directed and Undirected Models\\
\large Sriraam Natarajan, Tushar Khot, Daniel Lowd,\\Kristian Kersting, Prasad Tadepalli, and Jude Shavlik
}
\author{Harsha Kokel\\
The University of Texas at Dallas\\
harsha.kokel@utdallas.edu\\
\And
Nandini Ramanan\\
Indiana University, Bloomington\\
nramanan@indiana.edu\\
}

\maketitle
\begin{abstract}
\begin{quote}
Harsha Kokel and Nandini Ramanan present the work of Sriraam Natarajan, Tushar Khot, Daniel Lowd, Kristian Kersting, Prasad Tadepalli, and Jude Shavlik, \href{http://homes.soic.indiana.edu/natarasr/Papers/Natarajan.ECML10.pdf}{``Exploiting Causal Independence in Markov Logic Networks: Combining Undirected and Directed Models"}. Part IV in a presentation series on Representation as part of Professor Sriraam Natarajan's seminar on statistical relational learning (CS 7301.002). Work was presented on Wednesday, February 7, 2018 in ECSS 2.311 at the University of Texas at Dallas.
\end{quote}
\end{abstract}

\section{Summary}

The aim of this paper is twofold: remedy the representative disparities between directed and undirected models (specifically Bayesian Networks and Markov Logic Networks), and show how decomposable combination functions can be used to augment these in the relational setting.

1. What are decomposable combination functions and how do they differ from the wider set of all combination functions?
2. What is the algorithm that converts directed models to undirected models (touch on the cpt rules)?
3. How are the claims of this paper evaluated?

\section{Pros and Cons}

Pros:
Cons: Most notably, this method works best when the number of unobserved variables is small. Even when there is a single unobserved variable, the search space grows exponentially.

\section{Contributions}

The authors claim the following contributions: ``(1) A provable linear representation of decomposable combining functions within MLNs; (2) Explicit examples of average-based and noisy combination functions; (3) A formal description of the algorithm for converting from directed models with combining rules to MLNs; (4) A macro-definition that allows for succinct specification of the resulting MLNs; (5) Empirical proof that combining rules can improve the learning of MLNs when the domain knowledge available is minimal."

\section{Improvements}

\end{document}
