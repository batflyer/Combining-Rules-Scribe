%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing

% Additional Packages following those which are required.
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
}

\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Exploiting Causal Independence in Markov Logic Networks: Combining Directed and Undirected Models)
/Author (Harsha Kokel, Nandini Ramanan)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Exploiting Causal Independence in Markov Logic\\Networks: Combining Directed and Undirected Models\\
\large Sriraam Natarajan, Tushar Khot, Daniel Lowd,\\Kristian Kersting, Prasad Tadepalli, and Jude Shavlik
}
\author{Harsha Kokel\\
The University of Texas at Dallas\\
harsha.kokel@utdallas.edu\\
\And
Nandini Ramanan\\
Indiana University, Bloomington\\
nramanan@indiana.edu\\
}

\maketitle
\begin{abstract}
\begin{quote}
Harsha Kokel and Nandini Ramanan present the work of Sriraam Natarajan, Tushar Khot, Daniel Lowd, Kristian Kersting, Prasad Tadepalli, and Jude Shavlik, \href{http://homes.soic.indiana.edu/natarasr/Papers/Natarajan.ECML10.pdf}{``Exploiting Causal Independence in Markov Logic Networks: Combining Undirected and Directed Models"}. Part IV in a presentation series on Representation as part of Professor Sriraam Natarajan's seminar on statistical relational learning (CS 7301.002). Work was presented on Wednesday, February 7, 2018 in ECSS 2.311 at the University of Texas at Dallas.
\end{quote}
\end{abstract}

\section{Summary}

The aim of this paper is twofold: remedy the representative disparities between directed and undirected models (specifically Bayesian Networks and Markov Logic Networks) with causal independence, and show how decomposable combination functions can be used to augment these in the relational setting. Independence  of causal influences (``ICI'', from Heckerman) can be shown using directed models since they have a coherent joint distribution from which individual sensors can be aggregated. But this comes somewhat at the expense of more rigorous modeling requirements, perhaps most notably the inability to have cycles.

\subsection{Independence of Causal Influences}

Consider two domains: one in which we are considering the values produced by a set of \textit{N} sensors, and another where we are trying to conceive a student's overall satisfaction by observing observations about their grades and reviews of papers they published. Modeling a full conditional probability table (CPT) would normally require an order of $2^n$ values, but if one observes that individual measurements of each sensor are independent of each other one will see the root of the idea of causal independence: there are many values which can influence your target, but when each are likely to be independent of one another then learning the entire CPT is wasteful.

\subsection{Decomposable Combination Functions}

The authors extended Heckerman and Breese's definition of decomposable causal influence to the relational setting. Intuition for this idea is that a function is decomposable if it returns the same value regardless of the order that variables are observed in. Examples include aggregators such as \texttt{MEAN}, \texttt{MEDIAN}, \texttt{MODE}, \texttt{NOISY-OR}, \texttt{NOISY-AND}, etc. Whereas examples of nondecomposable functions may include \texttt{Difference}, \texttt{cross-correlation}, \texttt{first-m}, \texttt{m-of-n}, or other functions where the observation order changes your outcome.

\section{Pros and Cons}
This paper introduced a method to include causal independence in undirected models, introduced decomposable combining rules in Markov Logic Networks, and made compelling arguments about these being reasonable assumptions in medical domains. However, a notable drawback of this method is that it works best when the number of unobserved variables is small. Even when there is a single unobserved variable, the search space grows exponentially.

\section{Contributions}

The authors claim the following contributions: ``(1) A provable linear representation of decomposable combining functions within MLNs; (2) Explicit examples of average-based and noisy combination functions; (3) A formal description of the algorithm for converting from directed models with combining rules to MLNs; (4) A macro-definition that allows for succinct specification of the resulting MLNs; (5) Empirical proof that combining rules can improve the learning of MLNs when the domain knowledge available is minimal."

\section{Improvements}

The main change I would make if these experiments were re-implemented and this paper was written today, would be to expand on inference time of MLNs with and without combining rules. The authors explain that ``this translation from combining rules to MLNs is not without its cost. We found that the inference in the resulting MLN is 4-5 times slower than the one that does not use the combining rules.'' The authors go on to mention that specialized inference algorithms may be necessary as a point of future research.

\end{document}
